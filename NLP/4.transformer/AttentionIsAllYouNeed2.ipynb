{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdygM0QvGBVz"
   },
   "source": [
    "# 基于自注意力机制的翻译系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0REVxb6GBV0"
   },
   "source": [
    "## 1. 数据处理\n",
    "- 读取数据\n",
    "- 分别保存为inputs，outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMbb7XRqGBV1"
   },
   "outputs": [],
   "source": [
    "# with open('/tmp/nmt_data/train.en', 'r', encoding='utf8') as f:\n",
    "#     en_data = f.readlines()\n",
    "\n",
    "# with open('/tmp/nmt_data/train.zh', 'r', encoding='utf8') as f:     \n",
    "#     zh_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AIZ7VTlIGBV6",
    "outputId": "66810816-dae8-49c4-d0f0-9393fca6ce1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1050000it [00:07, 132704.33it/s]\n",
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.783 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "554616it [02:56,  2.89it/s]  "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import jieba\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "    \n",
    "with open('/tmp/nmt_data/train.en', 'r', encoding='utf8') as fi:\n",
    "    for line in tqdm(fi):\n",
    "        en = line.strip('\\n').strip('\\t').replace(',',' ,')[:-1].lower()\n",
    "        inputs.append(en.split(' '))\n",
    "    \n",
    "with open('/tmp/nmt_data/train.zh', 'r', encoding='utf8') as fo:  \n",
    "    for line in tqdm(fo):\n",
    "        zh = line.strip('\\n').strip('\\t')[:-1]\n",
    "#         outputs.append(jieba.lcut(zh))\n",
    "        outputs.extend(jieba.lcut(zh))\n",
    "        \n",
    "        \n",
    "        \n",
    "# for line in tqdm(en_data):\n",
    "#     en = line.strip('\\n').strip('\\t')\n",
    "#     inputs.append(en.replace(',',' ,')[:-1].lower())\n",
    "\n",
    "# for line in tqdm(zh_data):\n",
    "#     zh = line.strip('\\n').strip('\\t')\n",
    "#     outputs.append(zh[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q8s8l1tDGBV_",
    "outputId": "8b31a7b6-7237-4a52-8522-76d26abcb2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the show stars the x girls - a troupe of talented topless dancers , some of whom are classically trained', 'the centerpiece of the show is a farcical rendition of swan lake in which male and female performers dance in pink tutus and imitate swans', 'the removal of the barrier between performance and post-production was just as helpful for the actors', 'assist (somebody acting or reciting) by suggesting the next words of something forgotten or imperfectly learned', 'basically it was a fine performance i have only minor quibbles to make about her technique', \"after it's over , we watch the pairs of headlights glide in a neat line back up main street , dispersing as drivers turn off toward home\", 'after the performance they removed the back wall of the theatre and the cast summoned the audience onstage for an impromptu line dance', 'after the end of each performance with paper can be spread the water , light on the surface were saved. kids draw', 'after the performances , a garden party featuring delicious vegetarian food , which had been long awaited by many , finally began', 'the event featured a variety of spectacular performances , and was highlighted by the joyful exchange of special gifts']\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kFGSTmYsGBWC",
    "outputId": "1bb178c2-0cef-48ba-c527-7d0c27288d64",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['表演的明星是X女孩团队——由一对具有天才技艺的艳舞女孩们组成，其中有些人受过专业的训练', '表演的压轴戏是闹剧版《天鹅湖》，男女小人们身着粉红色的芭蕾舞裙扮演小天鹅', '表演和后期制作之间的屏障被清除了，这对演员来说一样大有裨益', '（表演或背诵时）通过暗示下面忘记或记地不准的东西来帮助某人', '表演基本上很精彩--我只对她的技巧稍有意见', '表演结束后，我们看到一对对车灯沿主路一路排回镇上，然后散开来各回各家', '表演结束后，移走了背景墙，随后全体演员即兴邀请观众上台齐跳并排舞', '表演结束后用宣纸轻铺水面，可将水面上的画进行拓印保存', '表演结束后，众人期待已久的园游会终于正式开锣，美味可口的素食佳肴让大家一饱口福', '表演节目丰富精采，交换礼物的欢乐时刻一到，则形成另一波高潮']\n"
     ]
    }
   ],
   "source": [
    "print(outputs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsyUGzxYGBWG"
   },
   "source": [
    "### 1.1 英文分词\n",
    "我们将英文用空格隔开即可，但是需要稍微修改一下，将大写字母全部用小写字母代替。在上文中使用`.lower`进行了替代。\n",
    "\n",
    "```py\n",
    "for line in tqdm(data):\n",
    "    [en, ch] = line.strip('\\n').split('\\t')\n",
    "    inputs.append(en[:-1].lower())\n",
    "    outputs.append(ch[:-1])\n",
    "\n",
    "```\n",
    "此处我们只需要将英文用空格分开即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Diqml7onGBWG"
   },
   "outputs": [],
   "source": [
    "inputs = [en.split(' ') for en in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "r35hryRpGBWL",
    "outputId": "62fd64c7-36b7-4964-9e15-0fe68deb7e39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'show', 'stars', 'the', 'x', 'girls', '-', 'a', 'troupe', 'of', 'talented', 'topless', 'dancers', ',', 'some', 'of', 'whom', 'are', 'classically', 'trained'], ['the', 'centerpiece', 'of', 'the', 'show', 'is', 'a', 'farcical', 'rendition', 'of', 'swan', 'lake', 'in', 'which', 'male', 'and', 'female', 'performers', 'dance', 'in', 'pink', 'tutus', 'and', 'imitate', 'swans'], ['the', 'removal', 'of', 'the', 'barrier', 'between', 'performance', 'and', 'post-production', 'was', 'just', 'as', 'helpful', 'for', 'the', 'actors'], ['assist', '(somebody', 'acting', 'or', 'reciting)', 'by', 'suggesting', 'the', 'next', 'words', 'of', 'something', 'forgotten', 'or', 'imperfectly', 'learned'], ['basically', 'it', 'was', 'a', 'fine', 'performance', 'i', 'have', 'only', 'minor', 'quibbles', 'to', 'make', 'about', 'her', 'technique'], ['after', \"it's\", 'over', ',', 'we', 'watch', 'the', 'pairs', 'of', 'headlights', 'glide', 'in', 'a', 'neat', 'line', 'back', 'up', 'main', 'street', ',', 'dispersing', 'as', 'drivers', 'turn', 'off', 'toward', 'home'], ['after', 'the', 'performance', 'they', 'removed', 'the', 'back', 'wall', 'of', 'the', 'theatre', 'and', 'the', 'cast', 'summoned', 'the', 'audience', 'onstage', 'for', 'an', 'impromptu', 'line', 'dance'], ['after', 'the', 'end', 'of', 'each', 'performance', 'with', 'paper', 'can', 'be', 'spread', 'the', 'water', ',', 'light', 'on', 'the', 'surface', 'were', 'saved.', 'kids', 'draw'], ['after', 'the', 'performances', ',', 'a', 'garden', 'party', 'featuring', 'delicious', 'vegetarian', 'food', ',', 'which', 'had', 'been', 'long', 'awaited', 'by', 'many', ',', 'finally', 'began'], ['the', 'event', 'featured', 'a', 'variety', 'of', 'spectacular', 'performances', ',', 'and', 'was', 'highlighted', 'by', 'the', 'joyful', 'exchange', 'of', 'special', 'gifts']]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jenFGFO0GBWN"
   },
   "source": [
    "### 1.2 中文分词\n",
    "- 中文分词选择结巴分词工具。\n",
    "```py\n",
    "import jieba\n",
    "outputs = [[char for char in jieba.cut(line) if char != ' '] for line in outputs]\n",
    "```\n",
    "- 也可以用hanlp。\n",
    "```py\n",
    "from pyhanlp import *\n",
    "outputs = [[term.word for term in HanLP.segment(line) if term.word != ' '] for line in outputs]\n",
    "```\n",
    "- 或者按字分词？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "a5r0uPPPGBWO",
    "outputId": "37a29150-ea61-4b94-8b40-29b9e3e2adbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.955 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['正如', '总是', '这样', '，', '一', '日食', '将', '带来', '真相', '浮出', '水面', '，', '并', '帮助', '你', '过去', '发生', '的', '事情'], ['正如', '总统', '演讲', '中', '所说', '，', '没有', '共和党', '的', '支持', '的话', '，', '想', '通过', '法律', '是', '不', '可能', '的'], ['正如', '总统', '在', '其', '就职演说', '中', '所说', '，', '我们', '将', '伸出手', '，', '但是', '他们', '也', '必须', '放松', '拳头'], ['正如', '最初', 'Gnutella', '用户', '所', '证明', '的', '那样', '，', '这个', '解决方案', '只是', '一定', '程度', '上', '有效'], ['正如', '最初', '索赔', '申请', '用例', '所', '提到', '的', '那样', '，', '本用例', '从', '税务', '或', '工资', '系统', '得到', '数据'], ['正如', '“', '最大', '能力', '”', '一样', '，', '我们', '能', '理解', '“', '余地', '”', '的', '意思', '但', '难以', '清楚', '定义', '它'], ['正如', '最近', '的', '事态', '发展', '所', '证明', '的', '那样', '，', '鉴于', '可能', '引发', '的', '灾难性', '后果', '，', '没有', '一个', '欧元区', '成员国', '会', '被', '允许', '出现', '违约'], ['正如', '最近', '的', '一些', '事件', '所示', '，', '小报', '仍然', '在', '报道', '中', '，', '尤其', '是', '在', '侵犯', '隐私', '方面', '，', '采用', '不', '雅', '作为'], ['正如', '最近', '广州', '街头', '的', '抗议', '活动', '所', '显示', '的', '那样', '，', '对此', '问题', '的', '讨论', '已', '远远', '超出', '了', '学术界', '的', '范畴'], ['正如', '最近', '南极', '冰层', '钻探', '透露', '的', '消息', '，', '由于', '化石', '燃料', '的', '排放', '，', '如今', '大气', '中', '的', '二氧化碳', '含量', '比', '至少', '一百万年', '内', '的', '任何', '时候', '都', '多', '了', '三分之一']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.enable_parallel(10)\n",
    "jieba_outputs = [[char for char in jieba.cut(line) if char != ' '] for line in outputs[-10:]]\n",
    "print(jieba_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9f5gw9AMGBWZ",
    "outputId": "966a121b-3fae-40b3-b07d-d18a477b1159"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1050000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 229/1050000 [00:00<07:39, 2283.48it/s]\u001b[A\n",
      "  0%|          | 533/1050000 [00:00<07:05, 2465.64it/s]\u001b[A\n",
      "  0%|          | 854/1050000 [00:00<06:36, 2648.08it/s]\u001b[A\n",
      "  0%|          | 1177/1050000 [00:00<06:14, 2798.27it/s]\u001b[A\n",
      "  0%|          | 1519/1050000 [00:00<05:54, 2959.60it/s]\u001b[A\n",
      "  0%|          | 1859/1050000 [00:00<05:40, 3078.93it/s]\u001b[A\n",
      "  0%|          | 2147/1050000 [00:00<06:37, 2636.15it/s]\u001b[A\n",
      "  0%|          | 2486/1050000 [00:00<06:10, 2824.52it/s]\u001b[A\n",
      "  0%|          | 2770/1050000 [00:00<06:33, 2663.12it/s]\u001b[A\n",
      "  0%|          | 3040/1050000 [00:01<06:41, 2606.58it/s]\u001b[A\n",
      "  0%|          | 3333/1050000 [00:01<06:28, 2695.28it/s]\u001b[A\n",
      "  0%|          | 3735/1050000 [00:01<05:49, 2990.49it/s]\u001b[A\n",
      "  0%|          | 4046/1050000 [00:01<05:52, 2968.12it/s]\u001b[A\n",
      "  0%|          | 4352/1050000 [00:01<05:57, 2921.00it/s]\u001b[A\n",
      "  0%|          | 4683/1050000 [00:01<05:45, 3026.35it/s]\u001b[A\n",
      "  0%|          | 4991/1050000 [00:01<06:05, 2857.04it/s]\u001b[A\n",
      "  1%|          | 5334/1050000 [00:01<05:47, 3007.76it/s]\u001b[A\n",
      "  1%|          | 5722/1050000 [00:01<05:23, 3224.42it/s]\u001b[A\n",
      "  1%|          | 6101/1050000 [00:02<05:09, 3374.75it/s]\u001b[A\n",
      "  1%|          | 6447/1050000 [00:02<05:16, 3299.68it/s]\u001b[A\n",
      "  1%|          | 6783/1050000 [00:02<05:18, 3274.36it/s]\u001b[A\n",
      "  1%|          | 7147/1050000 [00:02<05:09, 3374.55it/s]\u001b[A\n",
      "  1%|          | 7509/1050000 [00:02<05:02, 3443.95it/s]\u001b[A\n",
      "  1%|          | 7857/1050000 [00:02<05:31, 3145.08it/s]\u001b[A\n",
      "  1%|          | 8179/1050000 [00:02<05:29, 3164.74it/s]\u001b[A\n",
      "  1%|          | 8501/1050000 [00:02<05:29, 3163.27it/s]\u001b[A\n",
      "  1%|          | 8885/1050000 [00:02<05:11, 3337.65it/s]\u001b[A\n",
      "  1%|          | 9252/1050000 [00:02<05:03, 3427.86it/s]\u001b[A\n",
      "  1%|          | 9599/1050000 [00:03<05:04, 3416.68it/s]\u001b[A\n",
      "  1%|          | 9944/1050000 [00:03<05:16, 3283.43it/s]\u001b[A\n",
      "  1%|          | 10276/1050000 [00:03<05:52, 2950.46it/s]\u001b[A\n",
      "  1%|          | 10602/1050000 [00:03<05:42, 3036.47it/s]\u001b[A\n",
      "  1%|          | 10913/1050000 [00:03<05:54, 2931.88it/s]\u001b[A\n",
      "  1%|          | 11270/1050000 [00:03<05:35, 3097.16it/s]\u001b[A\n",
      "  1%|          | 11587/1050000 [00:03<05:33, 3117.25it/s]\u001b[A\n",
      "  1%|          | 11962/1050000 [00:03<05:16, 3282.57it/s]\u001b[A\n",
      "  1%|          | 12299/1050000 [00:03<05:13, 3307.18it/s]\u001b[A\n",
      "  1%|          | 12695/1050000 [00:04<04:58, 3478.69it/s]\u001b[A\n",
      "  1%|          | 13048/1050000 [00:04<05:06, 3387.40it/s]\u001b[A\n",
      "  1%|▏         | 13395/1050000 [00:04<05:03, 3411.71it/s]\u001b[A\n",
      "  1%|▏         | 13740/1050000 [00:04<05:04, 3406.40it/s]\u001b[A\n",
      "  1%|▏         | 14083/1050000 [00:04<05:08, 3357.84it/s]\u001b[A\n",
      "  1%|▏         | 14429/1050000 [00:04<05:05, 3386.78it/s]\u001b[A\n",
      "  1%|▏         | 14769/1050000 [00:04<05:06, 3382.10it/s]\u001b[A\n",
      "  1%|▏         | 15109/1050000 [00:04<05:05, 3384.25it/s]\u001b[A\n",
      "  1%|▏         | 15590/1050000 [00:04<04:38, 3713.92it/s]\u001b[A\n",
      "  2%|▏         | 15983/1050000 [00:04<04:34, 3773.64it/s]\u001b[A\n",
      "  2%|▏         | 16368/1050000 [00:05<04:43, 3643.28it/s]\u001b[A\n",
      "  2%|▏         | 16739/1050000 [00:05<05:27, 3157.10it/s]\u001b[A\n",
      "  2%|▏         | 17071/1050000 [00:05<05:32, 3103.13it/s]\u001b[A\n",
      "  2%|▏         | 17447/1050000 [00:05<05:15, 3274.46it/s]\u001b[A\n",
      "  2%|▏         | 17835/1050000 [00:05<05:00, 3433.46it/s]\u001b[A\n",
      "  2%|▏         | 18188/1050000 [00:05<05:04, 3392.52it/s]\u001b[A\n",
      "  2%|▏         | 18534/1050000 [00:05<05:15, 3273.93it/s]\u001b[A\n",
      "  2%|▏         | 18908/1050000 [00:05<05:03, 3400.07it/s]\u001b[A\n",
      "  2%|▏         | 19275/1050000 [00:05<04:56, 3475.89it/s]\u001b[A\n",
      "  2%|▏         | 19627/1050000 [00:06<04:56, 3477.20it/s]\u001b[A\n",
      "  2%|▏         | 19978/1050000 [00:06<05:06, 3365.65it/s]\u001b[A\n",
      "  2%|▏         | 20345/1050000 [00:06<04:58, 3450.44it/s]\u001b[A\n",
      "  2%|▏         | 20693/1050000 [00:06<05:05, 3368.25it/s]\u001b[A\n",
      "  2%|▏         | 21032/1050000 [00:06<05:23, 3178.25it/s]\u001b[A\n",
      "  2%|▏         | 21354/1050000 [00:06<06:52, 2495.11it/s]\u001b[A\n",
      "  2%|▏         | 21640/1050000 [00:06<06:36, 2593.18it/s]\u001b[A\n",
      "  2%|▏         | 22003/1050000 [00:06<06:02, 2835.67it/s]\u001b[A\n",
      "  2%|▏         | 22328/1050000 [00:06<05:48, 2947.30it/s]\u001b[A\n",
      "  2%|▏         | 22730/1050000 [00:07<05:20, 3203.49it/s]\u001b[A\n",
      "  2%|▏         | 23068/1050000 [00:07<05:31, 3097.94it/s]\u001b[A\n",
      "  2%|▏         | 23391/1050000 [00:07<05:39, 3019.47it/s]\u001b[A\n",
      "  2%|▏         | 23705/1050000 [00:07<05:36, 3054.15it/s]\u001b[A\n",
      "  2%|▏         | 24018/1050000 [00:07<05:34, 3068.81it/s]\u001b[A\n",
      "  2%|▏         | 24348/1050000 [00:07<05:27, 3134.21it/s]\u001b[A\n",
      "  2%|▏         | 24705/1050000 [00:07<05:15, 3251.97it/s]\u001b[A\n",
      "  2%|▏         | 25034/1050000 [00:07<06:02, 2827.07it/s]\u001b[A\n",
      "  2%|▏         | 25330/1050000 [00:07<06:14, 2732.51it/s]\u001b[A\n",
      "  2%|▏         | 25678/1050000 [00:08<05:50, 2920.50it/s]\u001b[A\n",
      "  2%|▏         | 26009/1050000 [00:08<05:38, 3025.55it/s]\u001b[A\n",
      "  3%|▎         | 26404/1050000 [00:08<05:14, 3253.43it/s]\u001b[A\n",
      "  3%|▎         | 26742/1050000 [00:08<05:11, 3289.87it/s]\u001b[A\n",
      "  3%|▎         | 27124/1050000 [00:08<04:58, 3432.13it/s]\u001b[A\n",
      "  3%|▎         | 27475/1050000 [00:08<05:34, 3060.54it/s]\u001b[A\n",
      "  3%|▎         | 27935/1050000 [00:08<05:00, 3401.97it/s]\u001b[A\n",
      "  3%|▎         | 28296/1050000 [00:08<05:07, 3320.02it/s]\u001b[A\n",
      "  3%|▎         | 28643/1050000 [00:08<05:05, 3346.08it/s]\u001b[A\n",
      "  3%|▎         | 28989/1050000 [00:09<05:17, 3212.51it/s]\u001b[A\n",
      "  3%|▎         | 29319/1050000 [00:09<05:28, 3108.35it/s]\u001b[A\n",
      "  3%|▎         | 29759/1050000 [00:09<04:59, 3408.38it/s]\u001b[A\n",
      "  3%|▎         | 30114/1050000 [00:09<04:56, 3443.33it/s]\u001b[A\n",
      "  3%|▎         | 30574/1050000 [00:09<04:33, 3723.06it/s]\u001b[A\n",
      "  3%|▎         | 30960/1050000 [00:09<04:35, 3693.38it/s]\u001b[A\n",
      "  3%|▎         | 31365/1050000 [00:09<04:29, 3785.39it/s]\u001b[A\n",
      "  3%|▎         | 31751/1050000 [00:09<04:31, 3753.28it/s]\u001b[A\n",
      "  3%|▎         | 32132/1050000 [00:09<04:38, 3650.79it/s]\u001b[A\n",
      "  3%|▎         | 32502/1050000 [00:10<05:22, 3154.60it/s]\u001b[A\n",
      "  3%|▎         | 32853/1050000 [00:10<05:12, 3252.14it/s]\u001b[A\n",
      "  3%|▎         | 33213/1050000 [00:10<05:03, 3349.09it/s]\u001b[A\n",
      "  3%|▎         | 33557/1050000 [00:10<05:21, 3164.40it/s]\u001b[A\n",
      "  3%|▎         | 33882/1050000 [00:10<05:31, 3069.60it/s]\u001b[A\n",
      "  3%|▎         | 34229/1050000 [00:10<05:20, 3168.95it/s]\u001b[A\n",
      "  3%|▎         | 34552/1050000 [00:10<05:21, 3156.87it/s]\u001b[A\n",
      "  3%|▎         | 34872/1050000 [00:10<05:28, 3093.05it/s]\u001b[A\n",
      "  3%|▎         | 35219/1050000 [00:10<05:20, 3164.76it/s]\u001b[A\n",
      "  3%|▎         | 35538/1050000 [00:11<05:27, 3099.35it/s]\u001b[A\n",
      "  3%|▎         | 35850/1050000 [00:11<05:34, 3034.91it/s]\u001b[A\n",
      "  3%|▎         | 36177/1050000 [00:11<05:26, 3100.43it/s]\u001b[A\n",
      "  3%|▎         | 36489/1050000 [00:11<05:34, 3034.43it/s]\u001b[A\n",
      "  4%|▎         | 36794/1050000 [00:11<06:20, 2662.52it/s]\u001b[A\n",
      "  4%|▎         | 37070/1050000 [00:11<07:07, 2369.06it/s]\u001b[A\n",
      "  4%|▎         | 37486/1050000 [00:11<06:12, 2720.25it/s]\u001b[A\n",
      "  4%|▎         | 37786/1050000 [00:11<06:05, 2769.80it/s]\u001b[A\n",
      "  4%|▎         | 38121/1050000 [00:11<05:46, 2921.21it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0078d7e5b506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moutpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mlcut\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlcut_for_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(self, sentence, cut_all, HMM)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcut_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jieba/__init__.py\u001b[0m in \u001b[0;36m__cut_DAG\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    250\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                             \u001b[0mrecognized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinalseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecognized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                                 \u001b[0;32myield\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jieba/finalseg/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mForce_Split_Words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jieba/finalseg/__init__.py\u001b[0m in \u001b[0;36m__cut\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnexti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# print pos_list, sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# outputs = [[char for char in jieba.cut(line) if char != ' '] for line in tqdm(outputs)]\n",
    "\n",
    "\n",
    "# output_tmp = []\n",
    "# for line in tqdm(outputs):\n",
    "#     output_tmp.append(jieba.lcut(line))\n",
    "# outpus = output_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fp-1z59YGBWd"
   },
   "source": [
    "### 1.3 生成字典\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "XPDmTQSFGBWe",
    "outputId": "e1f97b9e-a753-4327-c3d2-0542dbcc5ebd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20133/20133 [00:01<00:00, 14020.89it/s]\n",
      "100%|██████████| 20133/20133 [00:04<00:00, 4262.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(data, init=['<PAD>']):\n",
    "    vocab = init\n",
    "    for line in tqdm(data):\n",
    "        for word in line:\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    return vocab\n",
    "\n",
    "SOURCE_CODES = ['<PAD>']\n",
    "TARGET_CODES = ['<PAD>', '<GO>', '<EOS>']\n",
    "encoder_vocab = get_vocab(inputs, init=SOURCE_CODES)\n",
    "decoder_vocab = get_vocab(outputs, init=TARGET_CODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "IFhtgzuuGBWj",
    "outputId": "5d5c459b-8814-4cfe-aa8f-a065aad9c390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', 'hi', 'run', 'wait', 'hello', 'i', 'try', 'won', 'oh', 'no']\n",
      "['<PAD>', '<GO>', '<EOS>', '嗨', '你好', '你', '用', '跑', '的', '等等']\n"
     ]
    }
   ],
   "source": [
    "print(encoder_vocab[:10])\n",
    "print(decoder_vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UBCwOCKGBWl"
   },
   "source": [
    "### 1.4 数据生成器\n",
    "\n",
    "翻译系统解码器端需要输入和输出，因此解码器需要拼凑出输入序列和输出序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRhUa227GBWm"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = [[encoder_vocab.index(word) for word in line] for line in inputs]\n",
    "decoder_inputs = [[decoder_vocab.index('<GO>')] + [decoder_vocab.index(word) for word in line] for line in outputs]\n",
    "decoder_targets = [[decoder_vocab.index(word) for word in line] + [decoder_vocab.index('<EOS>')] for line in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "dxzUF9R0GBWp",
    "outputId": "8d736a7f-de4f-4184-8693-d5343dcc1c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3], [1, 4], [1, 5, 6, 7, 8], [1, 9]]\n",
      "[[3, 2], [4, 2], [5, 6, 7, 8, 2], [9, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_inputs[:4])\n",
    "print(decoder_targets[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITFlqyeIGBWu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_batch(encoder_inputs, decoder_inputs, decoder_targets, batch_size=4):\n",
    "    batch_num = len(encoder_inputs) // batch_size\n",
    "    for k in range(batch_num):\n",
    "        begin = k * batch_size\n",
    "        end = begin + batch_size\n",
    "        en_input_batch = encoder_inputs[begin:end]\n",
    "        de_input_batch = decoder_inputs[begin:end]\n",
    "        de_target_batch = decoder_targets[begin:end]\n",
    "        max_en_len = max([len(line) for line in en_input_batch])\n",
    "        max_de_len = max([len(line) for line in de_input_batch])\n",
    "        en_input_batch = np.array([line + [0] * (max_en_len-len(line)) for line in en_input_batch])\n",
    "        de_input_batch = np.array([line + [0] * (max_de_len-len(line)) for line in de_input_batch])\n",
    "        de_target_batch = np.array([line + [0] * (max_de_len-len(line)) for line in de_target_batch])\n",
    "        yield en_input_batch, de_input_batch, de_target_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "ad92b5GsGBWz",
    "outputId": "48f623da-e04d-4f70-b386-f19ba5cf0a45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3]]), array([[1, 3, 0, 0, 0],\n",
       "        [1, 4, 0, 0, 0],\n",
       "        [1, 5, 6, 7, 8],\n",
       "        [1, 9, 0, 0, 0]]), array([[3, 2, 0, 0, 0],\n",
       "        [4, 2, 0, 0, 0],\n",
       "        [5, 6, 7, 8, 2],\n",
       "        [9, 2, 0, 0, 0]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = get_batch(encoder_inputs, decoder_inputs, decoder_targets, batch_size=4)\n",
    "next(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FDZYnOuGBW3"
   },
   "source": [
    "## 2. 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgxTCsGdGBW3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlpHtw43GBW7"
   },
   "source": [
    "### 2.1 构造建模组件\n",
    "下面代码实现了图片结构中的各个功能组件。\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "#### layer norm层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUhfqeTXGBW8"
   },
   "outputs": [],
   "source": [
    "def normalize(inputs, \n",
    "              epsilon = 1e-8,\n",
    "              scope=\"ln\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "\n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "\n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ptiChtEGBW-"
   },
   "source": [
    "#### embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kATHZ4V0GBW-"
   },
   "outputs": [],
   "source": [
    "def embedding(inputs, \n",
    "              vocab_size, \n",
    "              num_units, \n",
    "              zero_pad=True, \n",
    "              scale=True,\n",
    "              scope=\"embedding\", \n",
    "              reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "\n",
    "    For example,\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]\n",
    "    ```    \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "\n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5) \n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IffFl8nhGBXB"
   },
   "source": [
    "#### multihead层\n",
    "该层实现了下面功能：\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4dVC9OaGBXC"
   },
   "outputs": [],
   "source": [
    "def multihead_attention(key_emb,\n",
    "                        que_emb,\n",
    "                        queries, \n",
    "                        keys, \n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "        \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(key_emb, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "        \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "   \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "         \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(que_emb, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "US_RmTCMGBXE"
   },
   "source": [
    "#### feedforward\n",
    "\n",
    "两层全连接，用卷积模拟加速运算，也可以使用dense层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFLKUuUmGBXF"
   },
   "outputs": [],
   "source": [
    "def feedforward(inputs, \n",
    "                num_units=[2048, 512],\n",
    "                scope=\"multihead_attention\", \n",
    "                reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIbOoVMFGBXH"
   },
   "source": [
    "#### label_smoothing.\n",
    "对于训练有好处，将0变为接近零的小数，1变为接近1的数，原文：\n",
    "\n",
    "During training, we employed label smoothing of value \u000fls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Su_os4AGBXI"
   },
   "outputs": [],
   "source": [
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```    \n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwSIPvYtGBXK"
   },
   "source": [
    "### 2.2 搭建模型\n",
    "模型实现下图结构：\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAkQDzSGGBXK"
   },
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self, is_training=True):\n",
    "        tf.reset_default_graph()\n",
    "        self.is_training = arg.is_training\n",
    "        self.hidden_units = arg.hidden_units\n",
    "        self.input_vocab_size = arg.input_vocab_size\n",
    "        self.label_vocab_size = arg.label_vocab_size\n",
    "        self.num_heads = arg.num_heads\n",
    "        self.num_blocks = arg.num_blocks\n",
    "        self.max_length = arg.max_length\n",
    "        self.lr = arg.lr\n",
    "        self.dropout_rate = arg.dropout_rate\n",
    "        \n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.y = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.de_inp = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        \n",
    "        # Encoder\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            # embedding\n",
    "            self.en_emb = embedding(self.x, vocab_size=self.input_vocab_size, num_units=self.hidden_units, scale=True, scope=\"enc_embed\")\n",
    "            self.enc = self.en_emb + embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
    "                                          vocab_size=self.max_length,num_units=self.hidden_units, zero_pad=False, scale=False,scope=\"enc_pe\")\n",
    "            ## Dropout\n",
    "            self.enc = tf.layers.dropout(self.enc, \n",
    "                                        rate=self.dropout_rate, \n",
    "                                        training=tf.convert_to_tensor(self.is_training))\n",
    "\n",
    "            ## Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    self.enc = multihead_attention(key_emb = self.en_emb,\n",
    "                                                   que_emb = self.en_emb,\n",
    "                                                   queries=self.enc, \n",
    "                                                    keys=self.enc, \n",
    "                                                    num_units=self.hidden_units, \n",
    "                                                    num_heads=self.num_heads, \n",
    "                                                    dropout_rate=self.dropout_rate,\n",
    "                                                    is_training=self.is_training,\n",
    "                                                    causality=False)\n",
    "\n",
    "            ### Feed Forward\n",
    "            self.enc = feedforward(self.enc, num_units=[4*self.hidden_units, self.hidden_units])\n",
    "        \n",
    "        # Decoder\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # embedding\n",
    "            self.de_emb = embedding(self.de_inp, vocab_size=self.label_vocab_size, num_units=self.hidden_units, scale=True, scope=\"dec_embed\")\n",
    "            self.dec = self.de_emb + embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.de_inp)[1]), 0), [tf.shape(self.de_inp)[0], 1]),\n",
    "                                          vocab_size=self.max_length,num_units=self.hidden_units, zero_pad=False, scale=False,scope=\"dec_pe\")\n",
    "            ## Dropout\n",
    "            self.dec = tf.layers.dropout(self.dec, \n",
    "                                        rate=self.dropout_rate, \n",
    "                                        training=tf.convert_to_tensor(self.is_training))        \n",
    "\n",
    "            ## Multihead Attention ( self-attention)\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    self.dec = multihead_attention(key_emb = self.de_emb,\n",
    "                                                   que_emb = self.de_emb,\n",
    "                                                   queries=self.dec, \n",
    "                                                    keys=self.dec, \n",
    "                                                    num_units=self.hidden_units, \n",
    "                                                    num_heads=self.num_heads, \n",
    "                                                    dropout_rate=self.dropout_rate,\n",
    "                                                    is_training=self.is_training,\n",
    "                                                    causality=True,\n",
    "                                                    scope='self_attention')\n",
    "\n",
    "            ## Multihead Attention ( vanilla attention)\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    self.dec = multihead_attention(key_emb = self.en_emb,\n",
    "                                                   que_emb = self.de_emb,\n",
    "                                                   queries=self.dec, \n",
    "                                                    keys=self.enc, \n",
    "                                                    num_units=self.hidden_units, \n",
    "                                                    num_heads=self.num_heads, \n",
    "                                                    dropout_rate=self.dropout_rate,\n",
    "                                                    is_training=self.is_training,\n",
    "                                                    causality=True,\n",
    "                                                    scope='vanilla_attention') \n",
    "\n",
    "            ### Feed Forward\n",
    "            self.outputs = feedforward(self.dec, num_units=[4*self.hidden_units, self.hidden_units])\n",
    "                \n",
    "        # Final linear projection\n",
    "        self.logits = tf.layers.dense(self.outputs, self.label_vocab_size)\n",
    "        self.preds = tf.to_int32(tf.argmax(self.logits, axis=-1))\n",
    "        self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
    "        tf.summary.scalar('acc', self.acc)\n",
    "                \n",
    "        if is_training:  \n",
    "            # Loss\n",
    "            self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=self.label_vocab_size))\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.y_smoothed)\n",
    "            self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "               \n",
    "            # Training Scheme\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "            self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "                   \n",
    "            # Summary \n",
    "            tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "            self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gO79HjmoGBXM"
   },
   "source": [
    "## 3. 训练模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upKSv_ylJ_tx"
   },
   "source": [
    "### 3.1 参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twQC0Km6GBXN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 02:17:34.805127 140052037289792 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_hparams():\n",
    "    params = tf.contrib.training.HParams(\n",
    "        num_heads = 8,\n",
    "        num_blocks = 6,\n",
    "        # vocab\n",
    "        input_vocab_size = 50,\n",
    "        label_vocab_size = 50,\n",
    "        # embedding size\n",
    "        max_length = 100,\n",
    "        hidden_units = 512,\n",
    "        dropout_rate = 0.2,\n",
    "        lr = 0.0003,\n",
    "        is_training = True)\n",
    "    return params\n",
    "\n",
    "        \n",
    "arg = create_hparams()\n",
    "arg.input_vocab_size = len(encoder_vocab)\n",
    "arg.label_vocab_size = len(decoder_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OO0OJOYwGBXP"
   },
   "source": [
    "### 3.2 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "QHmUtPZ4GBXR",
    "outputId": "9ca91084-e3c8-4908-a395-d34e5b995e69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 02:17:38.547116 140052037289792 deprecation.py:323] From <ipython-input-21-4660122af831>:28: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0726 02:17:38.633203 140052037289792 deprecation.py:323] From <ipython-input-18-524ea4e5bb4b>:35: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0726 02:17:39.140750 140052037289792 deprecation.py:323] From <ipython-input-18-524ea4e5bb4b>:56: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0726 02:17:40.417906 140052037289792 deprecation.py:323] From <ipython-input-19-45aea634a675>:21: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv1D` instead.\n",
      "W0726 02:17:43.564187 140052037289792 deprecation.py:323] From <ipython-input-21-4660122af831>:93: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0726 02:17:43.567326 140052037289792 deprecation.py:323] From <ipython-input-21-4660122af831>:94: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "100%|██████████| 314/314 [14:15<00:00,  4.37s/it]\n",
      "100%|██████████| 314/314 [13:36<00:00,  3.95s/it]\n",
      " 43%|████▎     | 136/314 [05:21<08:00,  2.70s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-aff646a81a12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdecoder_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mde_inp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "\n",
    "g = Graph(arg)\n",
    "\n",
    "saver =tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if os.path.exists('logs/model.meta'):\n",
    "        saver.restore(sess, 'logs/model')\n",
    "    writer = tf.summary.FileWriter('tensorboard/lm', tf.get_default_graph())\n",
    "    for k in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_num = len(encoder_inputs) // batch_size\n",
    "        batch = get_batch(encoder_inputs, decoder_inputs, decoder_targets, batch_size)\n",
    "        for i in tqdm(range(batch_num)):\n",
    "            encoder_input, decoder_input, decoder_target = next(batch)\n",
    "            feed = {g.x: encoder_input, g.y: decoder_target, g.de_inp:decoder_input}\n",
    "            cost,_ = sess.run([g.mean_loss,g.train_op], feed_dict=feed)\n",
    "            total_loss += cost\n",
    "            if (k * batch_num + i) % 10 == 0:\n",
    "                rs=sess.run(merged, feed_dict=feed)\n",
    "                writer.add_summary(rs, k * batch_num + i)\n",
    "        if (k+1) % 5 == 0:\n",
    "            print('epochs', k+1, ': average loss = ', total_loss/batch_num)\n",
    "    saver.save(sess, 'logs/model')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4JtOyPoGBXW"
   },
   "source": [
    "### 3.3 模型推断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "6yHefDhGGBXY",
    "outputId": "c75eb8a1-60c4-48f6-f7a7-f057cb081709"
   },
   "outputs": [],
   "source": [
    "arg.is_training = False\n",
    "\n",
    "g = Graph(arg)\n",
    "\n",
    "saver =tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'logs/model')\n",
    "    while True:\n",
    "        line = input('输入测试拼音: ')\n",
    "        if line == 'exit': break\n",
    "        line = line.lower().replace(',', ' ,').strip('\\n').split(' ')\n",
    "        x = np.array([encoder_vocab.index(pny) for pny in line])\n",
    "        x = x.reshape(1, -1)\n",
    "        de_inp = [[decoder_vocab.index('<GO>')]]\n",
    "        while True:\n",
    "            y = np.array(de_inp)\n",
    "            preds = sess.run(g.preds, {g.x: x, g.de_inp: y})\n",
    "            if preds[0][-1] == decoder_vocab.index('<EOS>'):\n",
    "                break\n",
    "            de_inp[0].append(preds[0][-1])\n",
    "        got = ''.join(decoder_vocab[idx] for idx in de_inp[0][1:])\n",
    "        print(got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qql1_AOlQnog"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K0REVxb6GBV0",
    "_FDZYnOuGBW3",
    "IlpHtw43GBW7",
    "iwSIPvYtGBXK",
    "upKSv_ylJ_tx"
   ],
   "name": "AttentionIsAllYouNeed.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
